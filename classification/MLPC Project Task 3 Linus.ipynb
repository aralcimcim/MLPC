{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TASK 3 Classification\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:57:59.851422Z",
     "start_time": "2025-05-18T08:57:58.347056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:57:59.910922Z",
     "start_time": "2025-05-18T08:57:59.864922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata = pd.read_csv('metadata.csv')\n",
    "print(f\"Shape of metadata: {metadata.shape}\")\n",
    "train_files = metadata[\"filename\"]\n",
    "\n",
    "features_dir = 'audio_features'\n",
    "labels_dir = 'labels'\n",
    "categories = ['Airplane', 'Alarm', 'Beep/Bleep', 'Bell', 'Bicycle', 'Bird Chirp', 'Bus', 'Car', 'Cat Meow',\n",
    "              'Chainsaw', 'Clapping', 'Cough', 'Cow Moo', 'Cowbell', 'Crying', 'Dog Bark', 'Doorbell', 'Drip',\n",
    "              'Drums', 'Fire', 'Footsteps', 'Guitar', 'Hammer', 'Helicopter', 'Hiccup', 'Horn Honk', 'Horse Neigh',\n",
    "              'Insect Buzz', 'Jackhammer', 'Laughter', 'Lawn Mower', 'Motorcycle', 'Piano', 'Pig Oink', 'Power Drill',\n",
    "              'Power Saw', 'Rain', 'Rooster Crow', 'Saxophone', 'Sewing Machine', 'Sheep/Goat Bleat', 'Ship/Boat',\n",
    "              'Shout', 'Singing', 'Siren', 'Sneeze', 'Snoring', 'Speech', 'Stream/River', 'Thunder', 'Train', 'Truck',\n",
    "              'Trumpet', 'Vacuum Cleaner', 'Violin', 'Washing Machine', 'Waves', 'Wind']\n",
    "print(f\"Amount of categories: {len(categories)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of metadata: (8230, 12)\n",
      "Amount of categories: 58\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aggregating labels for a frame when same class"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:58:00.204922Z",
     "start_time": "2025-05-18T08:58:00.201921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def aggregate_labels(file_labels):\n",
    "    \"\"\"\n",
    "    When a frame has multiple annotations for the same class, this function aggregates them\n",
    "    :param file_labels: list of lists\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    __y = []\n",
    "    for frame_labels in file_labels:\n",
    "        if (sum(frame_labels) == 0):\n",
    "            __y.append([0])\n",
    "        elif (np.count_nonzero(frame_labels) == len(frame_labels)):\n",
    "            __y.append([1])\n",
    "        else:  #The annotators don't agree on the label\n",
    "            __y.append([np.random.choice(frame_labels)])\n",
    "    return __y"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reading the files"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:58:00.213921Z",
     "start_time": "2025-05-18T08:58:00.210921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def read_files(file_names, num_to_read=1000):\n",
    "    X_train = []\n",
    "    Y_train = {}\n",
    "    for c in categories:\n",
    "        Y_train[c] = []\n",
    "    for f in file_names[:num_to_read]:  #we are not loading the entire dataset due to processing time\n",
    "        if not os.path.exists(os.path.join(features_dir, f.split('.')[0] + '.npz')):\n",
    "            continue\n",
    "        features = np.load(os.path.join(features_dir, f.split('.')[0] + '.npz'))[\"embeddings\"]\n",
    "        X_train.append(features)\n",
    "        y = np.load(os.path.join(labels_dir, f.split('.')[0] + '_labels.npz'))\n",
    "        for c in categories:\n",
    "            _y = aggregate_labels(y[c])\n",
    "            Y_train[c].extend(list(itertools.chain.from_iterable(_y)))\n",
    "    X_train = np.concatenate(X_train)\n",
    "    return X_train, Y_train\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.752496Z",
     "start_time": "2025-05-18T08:58:00.281421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read all files\n",
    "X_train, Y_train = read_files(train_files, len(train_files))\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of Y_train: {len(Y_train['Wind'])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1538577, 768)\n",
      "Shape of Y_train: 1538577\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline Classifier"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.787497Z",
     "start_time": "2025-05-18T08:59:22.783997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Baseline_classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.majority_class = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        '''x_train is a numpy array of features with shape NxD, where N is the number of datapoints and D the feature dimension\n",
    "        y_train is a list of binary labels in the shape Nx1\n",
    "        '''\n",
    "        # choose whatever is the most common label\n",
    "        self.majority_class = np.argmax(np.bincount(y_train))\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''x is a numpy array of features with shape NxD, where N is the number of datapoints and D the feature dimension\n",
    "        The function should return the predicted binary labels as a numpy array of shape Nx1\n",
    "        '''\n",
    "        # whenever the majority class is predicted, return 1, else return 0\n",
    "        return np.array([self.majority_class] * x.shape[0])  # return an array of shape Nx1\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.907996Z",
     "start_time": "2025-05-18T08:59:22.814997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Whether a sound is wind or not\n",
    "wind_x, wind_y = X_train, np.array(Y_train['Wind'])\n",
    "\n",
    "baseline = Baseline_classifier()\n",
    "baseline.fit(wind_x, wind_y.astype(int))\n",
    "\n",
    "y_train_pred = baseline.predict(wind_x)\n",
    "\n",
    "train_fraction_correct = np.mean(y_train_pred == wind_y)\n",
    "\n",
    "print(f\"Training Fraction Correct: {train_fraction_correct:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fraction Correct: 0.93\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Very imbalanced data\n",
    "## Plotting decision boundaries (for wind)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.913997Z",
     "start_time": "2025-05-18T08:59:22.911997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Plotting function\n",
    "# def plot_decision_boundary(knn, X, y, title, highlight_point=None):\n",
    "#     h = 0.1\n",
    "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                          np.arange(y_min, y_max, h))\n",
    "#     Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#\n",
    "#     plt.figure(figsize=(5, 3))\n",
    "#     plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.3)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=100, label='Train data')\n",
    "#     if highlight_point is not None:\n",
    "#         plt.scatter(highlight_point[0][0], highlight_point[0][1], color='gold', edgecolor='k',\n",
    "#                     marker='*', s=250, label='Test point (not in train)')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Feature 1\")\n",
    "#     plt.ylabel(\"Feature 2\")\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.929497Z",
     "start_time": "2025-05-18T08:59:22.927497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # Plotting decision boundaries\n",
    "#\n",
    "# knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# knn.fit(wind_x, wind_y)\n",
    "#\n",
    "# plot_decision_boundary(baseline, wind_x, wind_y, \"Baseline\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use train-test split and create confusion matrix"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.941996Z",
     "start_time": "2025-05-18T08:59:22.939996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Split data\n",
    "# X_train_wind, X_test_wind, y_train_wind, y_test_wind = train_test_split(\n",
    "#     wind_x, wind_y, test_size=0.3, random_state=42\n",
    "# )\n",
    "#\n",
    "# # Train model\n",
    "# knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# knn.fit(X_train_wind, y_train_wind)\n",
    "#\n",
    "# # Predictions\n",
    "# y_train_pred = knn.predict(X_train_wind)\n",
    "# y_test_pred = knn.predict(X_test_wind)\n",
    "#\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(8, 3))  # Side by side, smaller overall size\n",
    "#\n",
    "# # Train confusion matrix\n",
    "# cm_train = confusion_matrix(y_train_wind, y_train_pred)\n",
    "# disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=knn.classes_)\n",
    "# disp_train.plot(cmap='Blues', ax=axes[0], colorbar=False)\n",
    "# axes[0].set_title(\"Confusion Matrix - Train Set\")\n",
    "#\n",
    "# # Test confusion matrix\n",
    "# cm_test = confusion_matrix(y_test_wind, y_test_pred)\n",
    "# disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=knn.classes_)\n",
    "# disp_test.plot(cmap='Blues', ax=axes[1], colorbar=False)\n",
    "# axes[1].set_title(\"Confusion Matrix - Test Set\")\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.960996Z",
     "start_time": "2025-05-18T08:59:22.946996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_files = metadata.sample(len(metadata), random_state=42)[\"filename\"].unique()[:int(len(metadata) * 0.8)]\n",
    "validation_files = metadata.sample(len(metadata), random_state=42)[\"filename\"].unique()[\n",
    "                   int(len(metadata) * 0.8):int(len(metadata) * 0.9)]\n",
    "test_files = metadata.sample(len(metadata), random_state=42)[\"filename\"].unique()[int(len(metadata) * 0.9):]\n",
    "#So final split: 80% train, 10% val, 10% test\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(validation_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 6584\n",
      "Validation files: 823\n",
      "Test files: 823\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:59:22.970996Z",
     "start_time": "2025-05-18T08:59:22.968996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# y_true: ground truth labels\n",
    "# y_pred: predicted class labels (e.g., 0 or 1)\n",
    "# y_scores: predicted probabilities or decision function scores\n",
    "\n",
    "# accuracy      = accuracy_score(wind_y_test, y_test_pred)\n",
    "# precision     = precision_score(wind_y_test, y_test_pred, zero_division=0)\n",
    "# recall        = recall_score(wind_y_test, y_test_pred)\n",
    "# f1            = f1_score(wind_y_test, y_test_pred)\n",
    "# roc_auc       = roc_auc_score(wind_y_test, y_test_pred)\n",
    "# pr_auc        = average_precision_score(wind_y_test, y_test_pred)\n",
    "# weighted_acc  = balanced_accuracy_score(wind_y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Accuracy:         {accuracy:.3f}\")\n",
    "# print(f\"Weighted Accuracy:{weighted_acc:.3f}\")\n",
    "# print(f\"Precision:        {precision:.3f}\")\n",
    "# print(f\"Recall:           {recall:.3f}\")\n",
    "# print(f\"F1 Score:         {f1:.3f}\")\n",
    "# print(f\"ROC AUC:          {roc_auc:.3f}\")\n",
    "# print(f\"PR AUC:           {pr_auc:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Classifiers"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:00:41.478431Z",
     "start_time": "2025-05-18T08:59:22.984996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, Y_train = read_files(train_files, len(train_files)) # 80%\n",
    "X_val, Y_val = read_files(validation_files, len(validation_files)) # 10%\n",
    "X_test, Y_test = read_files(test_files, len(test_files)) # 10%\n",
    "\n",
    "# X_train, Y_train = read_files(train_files, 500)\n",
    "# X_val, Y_val = read_files(validation_files, 200)\n",
    "# X_test, Y_test = read_files(test_files, 200)\n",
    "\n",
    "#subsampling the training data to reduce run time\n",
    "# TODO: HERE YOU CAN CHANGE SAMPLE SIZE\n",
    "sample_size = len(X_train)\n",
    "indices = np.random.choice(len(X_train), size=sample_size, replace=False)\n",
    "\n",
    "X_train = X_train[indices]\n",
    "for c in categories:\n",
    "    Y_train[c] = [Y_train[c][i] for i in indices]\n",
    "\n",
    "# This is still wind\n",
    "wind_x_train, wind_y_train = X_train, np.array(Y_train['Wind'])\n",
    "wind_x_test, wind_y_test = X_test, np.array(Y_test['Wind'])\n",
    "wind_x_val, wind_y_val = X_val, np.array(Y_val['Wind'])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:00:41.496931Z",
     "start_time": "2025-05-18T09:00:41.494431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "# # Define the model\n",
    "# dt = DecisionTreeClassifier(random_state=42)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:00:41.520931Z",
     "start_time": "2025-05-18T09:00:41.518431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 10, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'criterion': ['gini', 'entropy']\n",
    "# }\n",
    "# best_score = 0\n",
    "# best_params = None\n",
    "# best_model = None\n",
    "#\n",
    "# # Generate all combinations of hyperparameters\n",
    "# for max_depth, min_samples_split, criterion in itertools.product(\n",
    "#     param_grid['max_depth'],\n",
    "#     param_grid['min_samples_split'],\n",
    "#     param_grid['criterion']\n",
    "# ):\n",
    "#     model = DecisionTreeClassifier(max_depth=max_depth,min_samples_split=min_samples_split,criterion=criterion,random_state=42)\n",
    "#     model.fit(wind_x_train, wind_y_train)\n",
    "#\n",
    "#     y_val_pred = model.predict(wind_x_val)\n",
    "#     score = balanced_accuracy_score(wind_y_val, y_val_pred)\n",
    "#     print(f\"Params: max_depth={max_depth}, min_samples_split={min_samples_split}, criterion={criterion} --> Accuracy: {score:.4f}\")\n",
    "#\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_params = {\n",
    "#             'max_depth': max_depth,\n",
    "#             'min_samples_split': min_samples_split,\n",
    "#             'criterion': criterion\n",
    "#         }\n",
    "#         best_model = model\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(best_params)\n",
    "# print(f\"Best Validation Accuracy: {best_score:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multilabel classification (what we need)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:00:44.353929Z",
     "start_time": "2025-05-18T09:00:41.657931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select all classes\n",
    "selected_classes = ['Airplane', 'Alarm', 'Beep/Bleep', 'Bell', 'Bicycle', 'Bird Chirp', 'Bus', 'Car', 'Cat Meow',\n",
    "        'Chainsaw', 'Clapping', 'Cough', 'Cow Moo', 'Cowbell', 'Crying', 'Dog Bark', 'Doorbell', 'Drip',\n",
    "        'Drums', 'Fire', 'Footsteps', 'Guitar', 'Hammer', 'Helicopter', 'Hiccup', 'Horn Honk', 'Horse Neigh',\n",
    "        'Insect Buzz', 'Jackhammer', 'Laughter', 'Lawn Mower', 'Motorcycle', 'Piano', 'Pig Oink', 'Power Drill',\n",
    "        'Power Saw', 'Rain', 'Rooster Crow', 'Saxophone', 'Sewing Machine', 'Sheep/Goat Bleat', 'Ship/Boat',\n",
    "        'Shout', 'Singing', 'Siren', 'Sneeze', 'Snoring', 'Speech', 'Stream/River', 'Thunder', 'Train', 'Truck',\n",
    "        'Trumpet', 'Vacuum Cleaner', 'Violin', 'Washing Machine', 'Waves', 'Wind']\n",
    "\n",
    "# selected_classes = ['Alarm', 'Wind', 'Dog Bark']\n",
    "\n",
    "y_train = np.array([Y_train[cls] for cls in selected_classes]).T\n",
    "y_val = np.array([Y_val[cls] for cls in selected_classes]).T\n",
    "y_test = np.array([Y_test[cls] for cls in selected_classes]).T"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-18T09:00:44.358929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, accuracy_score #, classification_report\n",
    "\n",
    "X = X_train\n",
    "Y = y_train\n",
    "\n",
    "# Fit classifier\n",
    "base_clf = DecisionTreeClassifier(random_state=42)\n",
    "br_clf = MultiOutputClassifier(base_clf, n_jobs=30)\n",
    "br_clf.fit(X, Y)\n",
    "\n",
    "# Predict\n",
    "Y_pred = br_clf.predict(X_val)\n",
    "\n",
    "# Compute balanced accuracy\n",
    "n_labels = y_val.shape[1]\n",
    "balanced_accuracies = [balanced_accuracy_score(y_val[:, i], Y_pred[:, i]) for i in range(n_labels)]\n",
    "balanced_accuracy_macro = np.mean(balanced_accuracies)\n",
    "print(\"Macro-Averaged Balanced Accuracy:\", balanced_accuracy_macro)\n",
    "\n",
    "# Classification report\n",
    "# print(classification_report(y_val, Y_pred, target_names=selected_classes, zero_division=0))\n",
    "\n",
    "# Macro F1\n",
    "f1_scores = [f1_score(y_val[:, i], Y_pred[:, i]) for i in range(n_labels)]\n",
    "print(\"Macro-Averaged F1 Score:\", np.mean(f1_scores))\n",
    "\n",
    "# Equal weight to each class, good when imbalanced\n",
    "macro_f1 = f1_score(y_val, Y_pred, average='macro')\n",
    "print(\"Macro-Averaged F1 Score:\", macro_f1)\n",
    "\n",
    "# Aggregates total TP, FP, and FN across all classes\n",
    "micro_f1 = f1_score(y_val, Y_pred, average='micro')\n",
    "print(\"Micro-Averaged F1 Score:\", micro_f1)\n",
    "\n",
    "# all labels for a sample must match exactly\n",
    "subset_acc = accuracy_score(y_val, Y_pred)\n",
    "print(\"Subset Accuracy (Exact Match Ratio):\", subset_acc)\n",
    "\n",
    "# Per class F1 score\n",
    "per_class_f1 = f1_score(y_val, Y_pred, average=None)\n",
    "for cls, score in zip(selected_classes, per_class_f1):\n",
    "    print(f\"{cls}: F1 Score = {score:.3f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "import joblib\n",
    "from datetime import datetime as dt\n",
    "minute = dt.now().time().minute\n",
    "hour = dt.now().time().hour\n",
    "\n",
    "joblib.dump(br_clf, f'MultiOutputClassifier{hour}-{minute}.joblib')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export/Import model and test on test set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:03:45.077586Z",
     "start_time": "2025-05-17T12:04:03.064751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "br_clf_loaded = joblib.load('MultiOutputClassifier2.joblib')\n",
    "\n",
    "# Use it to predict on the test set\n",
    "Y_test_pred = br_clf_loaded.predict(X_test)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, f1_score\n",
    "\n",
    "# Use the same selected_classes list to preserve order\n",
    "selected_classes = ['Alarm', 'Wind', 'Dog Bark']\n",
    "\n",
    "# True labels\n",
    "y_test = np.array([Y_test[cls] for cls in selected_classes]).T\n",
    "\n",
    "# Report\n",
    "print(classification_report(y_test, Y_test_pred, target_names=selected_classes, zero_division=0))\n",
    "balanced_accuracies = [balanced_accuracy_score(y_test[:, i], Y_test_pred[:, i]) for i in range(y_test.shape[1])]\n",
    "print(\"Macro-Averaged Balanced Accuracy (Test):\", np.mean(balanced_accuracies))\n",
    "print(\"Macro-Averaged F1 Score (Test):\",\n",
    "      np.mean([f1_score(y_test[:, i], Y_test_pred[:, i]) for i in range(y_test.shape[1])]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Alarm       0.10      0.09      0.09       393\n",
      "        Wind       0.21      0.15      0.18      3120\n",
      "    Dog Bark       0.41      0.52      0.46       547\n",
      "\n",
      "   micro avg       0.24      0.20      0.22      4060\n",
      "   macro avg       0.24      0.25      0.24      4060\n",
      "weighted avg       0.22      0.20      0.21      4060\n",
      " samples avg       0.02      0.02      0.02      4060\n",
      "\n",
      "Macro-Averaged Balanced Accuracy (Test): 0.6148565158349437\n",
      "Macro-Averaged F1 Score (Test): 0.24322226566988792\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:03:45.081085900Z",
     "start_time": "2025-05-17T11:12:17.776652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# n_labels = y_val.shape[1]\n",
    "# balanced_accuracies = []\n",
    "#\n",
    "# for i in range(n_labels):\n",
    "#     score = balanced_accuracy_score(y_val[:, i], Y_pred[:, i])\n",
    "#     balanced_accuracies.append(score)\n",
    "#\n",
    "# # Macro-average across labels\n",
    "# balanced_accuracy_macro = np.mean(balanced_accuracies)\n",
    "#\n",
    "# print(\"Macro-Averaged Balanced Accuracy:\", balanced_accuracy_macro)\n",
    "#\n",
    "#\n",
    "# print(classification_report(y_val, Y_pred, target_names=selected_classes, zero_division=0))\n",
    "# f1_scores = [f1_score(y_val[:, i], Y_pred[:, i]) for i in range(n_labels)]\n",
    "# print(\"Macro-Averaged F1 Score:\", np.mean(f1_scores))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Averaged Balanced Accuracy: 0.709200995571798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Linus\\Desktop\\Studium\\1. Semester\\Python\\3.11.7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 15\u001B[0m\n\u001B[0;32m     10\u001B[0m balanced_accuracy_macro \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(balanced_accuracies)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMacro-Averaged Balanced Accuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, balanced_accuracy_macro)\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mselected_classes\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     16\u001B[0m f1_scores \u001B[38;5;241m=\u001B[39m [f1_score(y_val[:, i], Y_pred[:, i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_labels)]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMacro-Averaged F1 Score:\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mmean(f1_scores))\n",
      "File \u001B[1;32m~\\Desktop\\Studium\\1. Semester\\Python\\3.11.7\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\Studium\\1. Semester\\Python\\3.11.7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2626\u001B[0m, in \u001B[0;36mclassification_report\u001B[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[0m\n\u001B[0;32m   2491\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   2492\u001B[0m     {\n\u001B[0;32m   2493\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2517\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2518\u001B[0m ):\n\u001B[0;32m   2519\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001B[39;00m\n\u001B[0;32m   2520\u001B[0m \n\u001B[0;32m   2521\u001B[0m \u001B[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2623\u001B[0m \u001B[38;5;124;03m    <BLANKLINE>\u001B[39;00m\n\u001B[0;32m   2624\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2626\u001B[0m     y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2628\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2629\u001B[0m         labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\n",
      "File \u001B[1;32m~\\Desktop\\Studium\\1. Semester\\Python\\3.11.7\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:112\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m    109\u001B[0m     y_type \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(y_type) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassification metrics can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt handle a mix of \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m targets\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    114\u001B[0m             type_true, type_pred\n\u001B[0;32m    115\u001B[0m         )\n\u001B[0;32m    116\u001B[0m     )\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001B[39;00m\n\u001B[0;32m    119\u001B[0m y_type \u001B[38;5;241m=\u001B[39m y_type\u001B[38;5;241m.\u001B[39mpop()\n",
      "\u001B[1;31mValueError\u001B[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass-multioutput targets"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using GPU (not working)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "#\n",
    "#\n",
    "# for i in range(y_train.shape[1]):\n",
    "#     vals = np.unique(y_train[:, i])\n",
    "#     if not np.all(np.isin(vals, [0, 1])):\n",
    "#         print(f\"Unexpected values in column {i}: {vals}\")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:03:45.087086100Z",
     "start_time": "2025-05-17T10:21:21.997506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sanitize labels\n",
    "# y_train = y_train.astype(np.int32)\n",
    "# y_val = y_val.astype(np.int32)\n",
    "#\n",
    "# # Define GPU-enabled base classifier\n",
    "# xgb_clf = XGBClassifier(tree_method='hist', eval_metric='logloss', verbosity=1, device='cuda')\n",
    "#\n",
    "# # Wrap in MultiOutputClassifier\n",
    "# multi_clf = MultiOutputClassifier(xgb_clf)\n",
    "#\n",
    "# # Fit\n",
    "# multi_clf.fit(X_train, y_train)\n",
    "#\n",
    "# # Predict and evaluate\n",
    "# y_pred = multi_clf.predict(X_val)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:03:45.088086Z",
     "start_time": "2025-05-17T10:21:22.005507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n_labels = y_val.shape[1]\n",
    "# balanced_accuracies = []\n",
    "#\n",
    "# for i in range(n_labels):\n",
    "#     score = balanced_accuracy_score(y_val[:, i], Y_pred[:, i])\n",
    "#     balanced_accuracies.append(score)\n",
    "#\n",
    "# # Macro-average across labels\n",
    "# balanced_accuracy_macro = np.mean(balanced_accuracies)\n",
    "#\n",
    "# print(\"Macro-Averaged Balanced Accuracy:\", balanced_accuracy_macro)\n",
    "#\n",
    "#\n",
    "# print(classification_report(y_val, Y_pred, target_names=selected_classes))\n",
    "# f1_scores = [f1_score(y_val[:, i], Y_pred[:, i]) for i in range(n_labels)]\n",
    "# print(\"Macro-Averaged F1 Score:\", np.mean(f1_scores))"
   ],
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
